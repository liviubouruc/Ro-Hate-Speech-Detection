{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/petru-\n",
      "[nltk_data]     liviubouruc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import tqdm\n",
    "import string\n",
    "import unidecode\n",
    "import spacy\n",
    "import phunspell\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('ro_core_news_sm')\n",
    "pspell = phunspell.Phunspell('ro_RO')\n",
    "stemmer = SnowballStemmer(\"romanian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>Minunat și înduioșător! Atitudinea acestui bie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633</th>\n",
       "      <td>Doar dvs ce mai amintiți de cei care au murit ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>Acuma vă stă pe creier himenul Mariei. Cei car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>Stimate domnule Cristian Tudor Popescu,vă mulț...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3968</th>\n",
       "      <td>Nu mai încape nici o îndoială că statul fură c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Response\n",
       "1528  Minunat și înduioșător! Atitudinea acestui bie...         0\n",
       "3633  Doar dvs ce mai amintiți de cei care au murit ...         0\n",
       "717   Acuma vă stă pe creier himenul Mariei. Cei car...         0\n",
       "1480  Stimate domnule Cristian Tudor Popescu,vă mulț...         0\n",
       "3968  Nu mai încape nici o îndoială că statul fură c...         0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('data.xlsx')\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stimat', 'dle', 'ctpdintotdeaun', 'stiut', 'gen', 'nebun', 'part', 'aceluias', 'intreg', 'simplu', 'teor', 'enunt', 'dvs', 'macar', 'original', 'auzit', 'pt', 'dat', 'teor', 'dezvirginar', 'interior', 'precum', 'ide', 'imposibilitatiiinseminar', 'cal', 'aeriean', 'mam', 'exasistent', 'medic', 'ani', '70', 'intro', 'disput', 'tat', 'tem', 'relig', 'mam', 'persoan', 'scolit', 'vrem', 'regel', 'miha', 'absolvent', 'postliceal', 'sanitar', 'tat', 'tanar', '7', 'ani', 'decat', 'taran', 'baragan', 'lacatus', 'baz', 'avsolvent', 'stefan', 'gheorghiu', 'grat', 'origin', 'sanat', 'timp', 'tat', 'om', 'simplu', 'probabil', 'bun', 'credinc', 'ocaz', 'cunosc', 'bin', 'murit', '56', 'ani', 'infarct', 'mam', 'traiest', '89', 'ani', 'intrun', 'azil', 'psihiatr', 'fiind', 'diagnostic', '1999', 'schizofren', 'paranoidaps', 'doresc', 'prezic', 'nus', 'mam', 'omid', 'detest', 'gen', 'expus', 'spet']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub('[^\\w\\s]', '', text)\n",
    "    \n",
    "    #tokens = word_tokenize(text)\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    tokens = [s for s in tokens if not s.isspace()]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('romanian')]\n",
    "    #tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # corrected_words = []\n",
    "    # for token in tokens:\n",
    "    #     if pspell.lookup(token):\n",
    "    #         for suggestion in pspell.suggest(token):\n",
    "    #             corrected_words.append(suggestion)\n",
    "    #             break\n",
    "    #     else:\n",
    "    #         corrected_words.append(stemmer.stem(token))\n",
    "    # corrected_words = [unidecode.unidecode(word) for word in corrected_words]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Exemplu de folosire\n",
    "input_text = \"Stimate d'le CTPdintotdeauna am știut că geniul și nebunia sunt părți ale aceluiași întreg. Cum așa? Simplu! Teoria enunțată aici de dvs nici măcar nu este originală! Am auzit pt prima dată teoria asta a dezvirginării din interior, precum și ideea imposibilitățiiinseminării pe cale aerieană la mama mea, ex-asistentă medicală, undeva prin anii '70, într-o dispută cu tata pe teme de religie! (Mama o persoanå școlită cam pe vremea Regelui Mihai, absolventă de Postliceală Sanitară; tata mai tânăr cu 7 ani decât ea, țăran din Bărăgan, lăcătuș la bază, dar avsolvent de Ștefan Gheorghiu grație ... \\\"originii sănătoase\\\"). Între timp, tata, un om simplu, dar probabil bun și credincios (nu am avut ocazia să îl cunosc prea bine) a murit (56 de ani, infarct); iar mama ....\\\"trăiește\\\" (89 de ani) într-un azil psihiatric, fiind diagnosticată (1999) cu schizofrenie paranoidă.P.S. Nu doresc să vă... \\\"prezic\\\" nimic (nu-s Mama Omida și oricum detest genul), doar am expus o speță.\"\n",
    "preprocessed_text = preprocess_text(input_text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False, tokenizer=preprocess_text, token_pattern=None)\n",
    "train_tfidf  = tfidf_vectorizer.fit_transform(train_data['Text'])\n",
    "test_tfidf  = tfidf_vectorizer.transform(test_data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.72       800\n",
      "           1       0.61      0.56      0.59       593\n",
      "\n",
      "    accuracy                           0.66      1393\n",
      "   macro avg       0.65      0.65      0.65      1393\n",
      "weighted avg       0.66      0.66      0.66      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SVC(C=3.5)\n",
    "model.fit(train_tfidf, train_data['Response'])\n",
    "print(metrics.classification_report(test_data['Response'], model.predict(test_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(lowercase=False, tokenizer=preprocess_text, token_pattern=None)\n",
    "train_bow  = bow_vectorizer.fit_transform(train_data['Text'])\n",
    "test_bow  = bow_vectorizer.transform(test_data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.77      0.74       818\n",
      "           1       0.63      0.55      0.59       575\n",
      "\n",
      "    accuracy                           0.68      1393\n",
      "   macro avg       0.67      0.66      0.66      1393\n",
      "weighted avg       0.68      0.68      0.68      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SVC(C=3.5)\n",
    "model.fit(train_bow, train_data['Response'])\n",
    "print(metrics.classification_report(test_data['Response'], model.predict(test_bow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(\"model_lwcase_no_diac.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Text'] = train_data['Text'].apply(preprocess_text)\n",
    "test_data['Text'] = test_data['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape: (14325, 300)\n",
      "Epoch 1/10\n",
      "174/174 [==============================] - ETA: 0s - loss: 0.6507 - accuracy: 0.6234\n",
      "Epoch 1: val_accuracy improved from -inf to 0.64752, saving model to best_model_weights.keras\n",
      "174/174 [==============================] - 35s 194ms/step - loss: 0.6507 - accuracy: 0.6234 - val_loss: 0.6201 - val_accuracy: 0.6475 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.7604\n",
      "Epoch 2: val_accuracy improved from 0.64752 to 0.66547, saving model to best_model_weights.keras\n",
      "174/174 [==============================] - 33s 189ms/step - loss: 0.5023 - accuracy: 0.7604 - val_loss: 0.6383 - val_accuracy: 0.6655 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.8820\n",
      "Epoch 3: val_accuracy did not improve from 0.66547\n",
      "174/174 [==============================] - 33s 191ms/step - loss: 0.2823 - accuracy: 0.8820 - val_loss: 0.7462 - val_accuracy: 0.6655 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "174/174 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9316\n",
      "Epoch 4: val_accuracy did not improve from 0.66547\n",
      "174/174 [==============================] - 34s 196ms/step - loss: 0.1719 - accuracy: 0.9316 - val_loss: 1.1048 - val_accuracy: 0.6511 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "174/174 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9490\n",
      "Epoch 5: val_accuracy did not improve from 0.66547\n",
      "174/174 [==============================] - 33s 190ms/step - loss: 0.1253 - accuracy: 0.9490 - val_loss: 1.4890 - val_accuracy: 0.6346 - lr: 0.0010\n",
      "44/44 [==============================] - 2s 41ms/step - loss: 1.4890 - accuracy: 0.6346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4890155792236328, 0.6346015930175781]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Conv1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train_data['Text']))\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "#%%\n",
    "train_input_seq = tokenizer.texts_to_sequences(train_data['Text'])\n",
    "test_input_seq = tokenizer.texts_to_sequences(test_data['Text'])\n",
    "\n",
    "max_sequence_length = max(len(seq) for seq in train_input_seq)\n",
    "\n",
    "train_padded_seq = pad_sequences(train_input_seq, maxlen=max_sequence_length)\n",
    "test_padded_seq = pad_sequences(test_input_seq, maxlen=max_sequence_length)\n",
    "\n",
    "#%%\n",
    "embedding_dim = 300\n",
    "\n",
    "embedding_matrix = np.zeros((total_words, embedding_dim))\n",
    "\n",
    "for word, token in tokenizer.word_index.items():\n",
    "        if word in wv_from_bin:\n",
    "                embedding_matrix[token] = wv_from_bin[word]\n",
    "\n",
    "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)\n",
    "\n",
    "#%%\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
    "model.add(Conv1D(30, 5, activation='relu'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=3),\n",
    "             ModelCheckpoint(\"best_model_weights.keras\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)]\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_padded_seq, train_data['Response'], epochs=10, verbose=1, validation_data=(test_padded_seq, test_data['Response']), callbacks=callbacks)\n",
    "model.evaluate(test_padded_seq, test_data['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 2s 38ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.87      0.77       818\n",
      "           1       0.70      0.45      0.55       575\n",
      "\n",
      "    accuracy                           0.69      1393\n",
      "   macro avg       0.70      0.66      0.66      1393\n",
      "weighted avg       0.69      0.69      0.68      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"best_model_weights.keras\")\n",
    "print(metrics.classification_report(test_data['Response'], (model.predict(test_padded_seq) >= 0.5).astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in text.split()]\n",
    "    return stemmed_tokens\n",
    "\n",
    "TF-IDF SVM 0.63, 0.63\n",
    "BoW SVM 0.63, 0.62\n",
    "--------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('romanian')]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "TF-IDF SVM 0.65, 0.65\n",
    "BoW SVM 0.64, 0.63\n",
    "--------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('romanian')]\n",
    "\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "    \n",
    "TF-IDF SVM 0.63, 0.63\n",
    "BoW SVM 0.64, 0.62\n",
    "--------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    #tokens = word_tokenize(text)\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('romanian')]\n",
    "\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "    \n",
    "TF-IDF SVM 0.65, 0.64\n",
    "BoW SVM 0.65, 0.63\n",
    "--------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    #tokens = word_tokenize(text)\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('romanian')]\n",
    "\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens \n",
    "    \n",
    "TF-IDF SVM 0.66, 0.66\n",
    "BoW SVM 0.66, 0.65\n",
    "--------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    #tokens = word_tokenize(text)\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('romanian')]\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "TF-IDF SVM 0.65, 0.65\n",
    "BoW SVM 0.65, 0.65\n",
    "--------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    #text = unidecode.unidecode(text)\n",
    "    \n",
    "    #tokens = word_tokenize(text)\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('romanian')]\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    corrected_words = []\n",
    "    for token in tokens:\n",
    "        if pspell.lookup(token):\n",
    "            for suggestion in pspell.suggest(token):\n",
    "                corrected_words.append(suggestion)\n",
    "                break\n",
    "        else:\n",
    "            corrected_words.append(stemmer.stem(token))\n",
    " \n",
    " TF-IDF SVM 0.63, 0.63 (50 min)\n",
    " ----------------------------------------------\n",
    " def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub('[^\\w\\s]', '', text)\n",
    "    \n",
    "    #tokens = word_tokenize(text)\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('romanian')]\n",
    "    #tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # corrected_words = []\n",
    "    # for token in tokens:\n",
    "    #     if pspell.lookup(token):\n",
    "    #         for suggestion in pspell.suggest(token):\n",
    "    #             corrected_words.append(suggestion)\n",
    "    #             break\n",
    "    #     else:\n",
    "    #         corrected_words.append(stemmer.stem(token))\n",
    "    # corrected_words = [unidecode.unidecode(word) for word in corrected_words]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "    \n",
    "TF-IDF SVM 0.68, 0.68 (cu lemma 0.66)\n",
    "BoW SVM 0.68 0.67\n",
    "\n",
    "-------------------\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub('[^\\w\\s]', '', text)\n",
    "    \n",
    "    #tokens = word_tokenize(text)\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    tokens = [s for s in tokens if not s.isspace()]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('romanian')]\n",
    "    #tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # corrected_words = []\n",
    "    # for token in tokens:\n",
    "    #     if pspell.lookup(token):\n",
    "    #         for suggestion in pspell.suggest(token):\n",
    "    #             corrected_words.append(suggestion)\n",
    "    #             break\n",
    "    #     else:\n",
    "    #         corrected_words.append(stemmer.stem(token))\n",
    "    # corrected_words = [unidecode.unidecode(word) for word in corrected_words]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "Word2Vec nisioi + RNN 0.69\n",
    "\n",
    "cu lemma si 300 la bilstm -> 0.67\n",
    "cu lemma si 30 la bilstm -> 0.66\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
